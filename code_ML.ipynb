{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cb503cf",
   "metadata": {},
   "source": [
    "This Code is written to implement two different machine learning algorithms for text classification.I have used a dataset of ML paper peer reviews from the International Conference of Learning Representation. Different performance metrices are visualised to compare the efficiency of the Algortihms for the given dataset.\n",
    "Linear Support Vector Machine and Random Forest Classifier are trained with the given ML paper reviews dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273ec2b2",
   "metadata": {},
   "source": [
    "Importing all the necessary packages and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5aea1fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Rabin Thapa\n",
      "[nltk_data]     Kshetry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Rabin Thapa\n",
      "[nltk_data]     Kshetry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a24b319",
   "metadata": {},
   "source": [
    "Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae1649fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('C:/Users/Rabin Thapa Kshetry/Desktop/Term 2/AML/dataset/processed_reviews_split_surnamesABCD_minimal.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e95cd1",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis and Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52389304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6119\n"
     ]
    }
   ],
   "source": [
    "# displaying the number of records in the dataset\n",
    "print(len(dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d709bbb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>confidence_score</th>\n",
       "      <th>review_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3318.000000</td>\n",
       "      <td>6119.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.795057</td>\n",
       "      <td>4.689492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.815260</td>\n",
       "      <td>2.192145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       confidence_score  review_score\n",
       "count       3318.000000   6119.000000\n",
       "mean           3.795057      4.689492\n",
       "std            0.815260      2.192145\n",
       "min            1.000000     -1.000000\n",
       "25%            3.000000      3.000000\n",
       "50%            4.000000      5.000000\n",
       "75%            4.000000      6.000000\n",
       "max            5.000000     10.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# displaying the statistical description of numerical attributes from the given dataset\n",
    "dataframe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "606f6349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                  object\n",
       "confidence_score     float64\n",
       "review_score         float64\n",
       "acceptance_status     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# displaying the datatypes of all the attributes from the given dataset\n",
    "dataframe.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4137fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271\n"
     ]
    }
   ],
   "source": [
    "# identifying the duplicate records\n",
    "print(dataframe.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "041589cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5848\n"
     ]
    }
   ],
   "source": [
    "# Dropping all the duplicates from the dataset and storing in the new dataframe dataframe1\n",
    "dataframe1= dataframe.drop_duplicates()\n",
    "print(len(dataframe1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e11742a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231\n"
     ]
    }
   ],
   "source": [
    "# identifying the duplicate records for the text column specifically\n",
    "print(dataframe1.text.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abe0aa37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5617\n"
     ]
    }
   ],
   "source": [
    "# Dropping all the duplicates from the dataset focussing in text column and storing in the new dataframe dataframe2\n",
    "dataframe2= dataframe1.drop_duplicates(subset='text')\n",
    "print(len(dataframe2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67195919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3035\n"
     ]
    }
   ],
   "source": [
    "# Dropping all the records having missing values or labels and storing in the new dataframe dataframe3\n",
    "dataframe3=dataframe2.dropna()\n",
    "print(len(dataframe3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85b884a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2992\n"
     ]
    }
   ],
   "source": [
    "# Dropping all the records having invalid value for review_score column and storing in dataframe4\n",
    "dataframe4 = dataframe3.drop(dataframe3.index[dataframe3['review_score'] == -1])\n",
    "print(len(dataframe4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee8bc0f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>confidence_score</th>\n",
       "      <th>review_score</th>\n",
       "      <th>acceptance_status</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>iclr_review_0000</th>\n",
       "      <td>Predicting Medications from Diagnostic Codes w...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Accept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iclr_review_0003</th>\n",
       "      <td>Sufficient Conditions for Robustness to Advers...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Reject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iclr_review_0004</th>\n",
       "      <td>Unsupervised Learning of the Set of Local Maxi...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Accept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iclr_review_0005</th>\n",
       "      <td>Global Optimality Conditions for Deep Neural N...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Accept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iclr_review_0006</th>\n",
       "      <td>Gaussian Process Behaviour in Wide Deep Neural...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Accept</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               text  \\\n",
       "review_id                                                             \n",
       "iclr_review_0000  Predicting Medications from Diagnostic Codes w...   \n",
       "iclr_review_0003  Sufficient Conditions for Robustness to Advers...   \n",
       "iclr_review_0004  Unsupervised Learning of the Set of Local Maxi...   \n",
       "iclr_review_0005  Global Optimality Conditions for Deep Neural N...   \n",
       "iclr_review_0006  Gaussian Process Behaviour in Wide Deep Neural...   \n",
       "\n",
       "                  confidence_score  review_score acceptance_status  \n",
       "review_id                                                           \n",
       "iclr_review_0000               3.0           6.0            Accept  \n",
       "iclr_review_0003               3.0           5.0            Reject  \n",
       "iclr_review_0004               3.0           8.0            Accept  \n",
       "iclr_review_0005               4.0           7.0            Accept  \n",
       "iclr_review_0006               4.0           6.0            Accept  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the top 5 rows of the cleaned dataframe \n",
    "dataframe4.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56b4b316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>confidence_score</th>\n",
       "      <th>review_score</th>\n",
       "      <th>acceptance_status</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>iclr_review_6108</th>\n",
       "      <td>Improving Composition of Sentence Embeddings t...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Reject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iclr_review_6109</th>\n",
       "      <td>Neural Tree Transducers for Tree to Tree Learn...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Reject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iclr_review_6112</th>\n",
       "      <td>Auxiliary Guided Autoregressive Variational Au...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Reject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iclr_review_6116</th>\n",
       "      <td>Learning to Augment Influential Data. Data aug...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Reject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iclr_review_6117</th>\n",
       "      <td>The Incredible Shrinking Neural Network: New P...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Reject</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               text  \\\n",
       "review_id                                                             \n",
       "iclr_review_6108  Improving Composition of Sentence Embeddings t...   \n",
       "iclr_review_6109  Neural Tree Transducers for Tree to Tree Learn...   \n",
       "iclr_review_6112  Auxiliary Guided Autoregressive Variational Au...   \n",
       "iclr_review_6116  Learning to Augment Influential Data. Data aug...   \n",
       "iclr_review_6117  The Incredible Shrinking Neural Network: New P...   \n",
       "\n",
       "                  confidence_score  review_score acceptance_status  \n",
       "review_id                                                           \n",
       "iclr_review_6108               3.0           5.0            Reject  \n",
       "iclr_review_6109               4.0           7.0            Reject  \n",
       "iclr_review_6112               4.0           5.0            Reject  \n",
       "iclr_review_6116               4.0           6.0            Reject  \n",
       "iclr_review_6117               4.0           3.0            Reject  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the bottom 5 rows of the cleaned dataframe \n",
    "dataframe4.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eef6e026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the cleaned dataframe into a csv file in the local disk as exclusion4.csv\n",
    "dataframe4.to_csv('C:/Users/Rabin Thapa Kshetry/Desktop/Term 2/AML/exclusion4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce0e88c",
   "metadata": {},
   "source": [
    "Target Label distribution into acceptance_status and review_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "49117f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Predicting Medications from Diagnostic Codes with Recurrent Neural Networks. It is a surprising fact that electronic medical records are failing at one of their primary purposes, that of tracking the set of medications that the patient is actively taking. Studies estimate that up to 50% of such lists omit active drugs, and that up to 25% of all active medications do not appear on the appropriate patient list. Manual efforts to maintain these lists involve a great deal of tedious human labor, which could be reduced by computational tools to suggest likely missing or incorrect medications on a patient’s list. We report here an application of recurrent neural networks to predict the likely therapeutic classes of medications that a patient is taking, given a sequence of the last 100 billing codes in their record. Our best model was a GRU that achieved high prediction accuracy (micro-averaged AUC 0.93, Label Ranking Loss 0.076), limited by hardware constraints on model size. Additionally, examining individual cases revealed that many of the predictions marked incorrect were likely to be examples of either omitted medications or omitted billing codes, supporting our assertion of a substantial number of errors and omissions in the data, and the likelihood of models such as these to help correct them.. This is a well-conducted and well-written study on the prediction of medication from diagnostic codes. The authors compared GRUs, LSTMs, feed-forward networks and random forests (making a case for why random forests should be used, instead of SVMs) and analysed the predictions and embeddings. The authors also did address the questions of the reviewers. My only negative point is that this work might be more relevant for a data science or medical venue rather than at ICLR.. This paper applies RNNs to predict medications from billing costs. While this paper does not have technical novelty, it is well done and well organized. It demonstrates a creative use of recent models in a very important domain, and I think many people in our community are interested and inspired by well-done applications that branch to socially important domains. Moreover, I think an advantage to accepting it at ICLR is that it gives our *expert* stamp of approval -- I see a lot of questionable / badly applied / antiquated machine learning methods in domain conferences, so I think it would be helpful for those domains to have examples of application papers that are considered sound.']\n",
      " ['Sufficient Conditions for Robustness to Adversarial Examples: a Theoretical and Empirical Study with Bayesian Neural Networks. We prove, under two sufficient conditions, that idealised models can have no adversarial examples. We discuss which idealised models satisfy our conditions, and show that idealised Bayesian neural networks (BNNs) satisfy these. We continue by studying near-idealised BNNs using HMC inference, demonstrating the theoretical ideas in practice. We experiment with HMC on synthetic data derived from MNIST for which we know the ground-truth image density, showing that near-perfect epistemic uncertainty correlates to density under image manifold, and that adversarial images lie off the manifold in our setting. This suggests why MC dropout, which can be seen as performing approximate inference, has been observed to be an effective defence against adversarial examples in practice; We highlight failure-cases of non-idealised BNNs relying on dropout, suggesting a new attack for dropout models and a new defence as well. Lastly, we demonstrate the defence on a cats-vs-dogs image classification task with a VGG13 variant.. In this paper, the authors posit a class of discriminative Bayesian classifiers that, under sufficient conditions, do not have any adversarial examples. They distinguish between two sources of uncertainty (epistemic and aleatoric), and show that mutual information is a measure of epistemic uncertainty (essentially uncertainty due to missing regions of the input space). They then define an idealised Bayesian Neural Network (BNN), which is essentially a BNN that 1) outputs the correct class probability (and always with probability 1.0) for each input in the training set (and for each transformation of training inputs that the data distribution is invariant under), and 2) outputs a sufficiently high uncertainty for each input not in the union of delta-balls surrounding the training set points. Similarly, an example is defined to be adversarial if it has two characteristics: it 1) lies far from the training data but is classified with high output probability, and it 2) is classified with high output probability although it lies very close to another example that is classified with high output probability for the other class. Condition 1) of an idealised BNN prevents Definition 2) of an adversarial example using the fact that BNNs are continuous, and Condition 2) prevents Definition 1) of an adversarial example since it will prevent \"garbage\" examples by predicting with high uncertainty (of course I*m glossing over many important technical details, but these are the main ideas if I understand correctly). The authors backed up their theoretical findings with empirical experiments. In the synthetic MNIST examples, the authors show that adversarial attacks are indeed correlated with lower true input probability. They show that training with HMC results in high uncertainty for inputs not near the input-space, a quality certainly not shared with all other deep models (and another reason that Bayesian models should be preferred for preventing adversarial attacks). On the other hand, the out-of-sample uncertainty for training with dropout is not sufficient to prevent adversarial attacks, although the authors posit a form of dropout ensemble training to help prevent these vulnerabilities. The authors are tackling an important issue with theoretical and technical tools that are not used often enough in machine learning research. Much of the literature on adversarial attacks is focused on finding adversarial examples, without trying to find a unifying theory for why they work. They do a very solid exposition of previous work, and one of the strengths of this paper comes in presenting their findings in the context of previously discovered adversarial attacks, in particular that of the spheres data set. Ultimately, I*m not convinced of the usefulness of their theoretical findings. In particular, the assumption that the model is invariant to all transformations that the data distribution is invariant under is an unprovable assumption that can expose many real-world vulnerabilities. This is the case of the spheres data set without a rotation invariance from Gilmer et al. (2018). In the appendix, the authors mention that the data invariance property is key for making the proof non-vacuous, and I would agree. Without the data invariance property, the proof mainly relies on the fact that BNNs are continuous. The experiments are promising in support of the theory, but they do not seem to address this data invariance property. Indeed a model can prevent adversarial examples by predicting high uncertainty for all points that are not near the training examples, which Bayesian models are well equipped to do. I also thought the paper was unclear at times. It is not easy to write such a technical and theoretical paper and to clearly convey all the main points, but I think the paper would*ve benefited from more clarity. For example, the notation is overloaded in a way that made it difficult to understand the main proofs, such as not clearly explaining what is meant by I(w ; p) and not contrasting between the binary entropy function used for the entropy of a constant epsilon and the more general random variable entropy function. In contrast, I thought the appendices were clearer and helpful for explaining the main ideas. Additionally, Lemma 1 follows trivially from continuity of a BNN. Perhaps removing this and being clearer with notation would*ve allowed for more room to be clearer for the proof of Theorem 1. A more minor point that I think would be interesting is comparing training with HMC to training with variational inference. Do the holes that come from training with dropout still exist for VI? VI could certainly scale in a way that HMC could not, which perhaps would make the results more applicable. Overall, this is a promising theoretical paper although I*m not currently convinced of the real-world applications beyond the somewhat small examples in the experiments section. PROS -Importance of the issue -Exposition and relation to previous work -Experimental results (although these were for smaller data sets) -Appendices really helped aid the understanding CONS -Real world usefulness -Clarity. ']\n",
      " ['Unsupervised Learning of the Set of Local Maxima. This paper describes a new form of unsupervised learning, whose input is a set of unlabeled points that are assumed to be local maxima of an unknown value function in an unknown subset of the vector space. Two functions are learned: (i) a set indicator , which is a binary classifier, and (ii) a comparator function that given two nearby samples, predicts which sample has the higher value of the unknown function . Loss terms are used to ensure that all training samples are a local maxima of , according to and satisfy . Therefore, and provide training signals to each other: a point in the vicinity of satisfies or is deemed by to be lower in value than . We present an algorithm, show an example where it is more efficient to use local maxima as an indicator function than to employ conventional classification, and derive a suitable generalization bound. Our experiments show that the method is able to outperform one-class classification algorithms in the task of anomaly detection and also provide an additional signal that is extracted in a completely unsupervised way.. The reviewer feels that the paper is hard to follow. The abstract is confusing enough and raises a number of questions. The paper talks about `\"local maxima\" without defining an optimization problem. What is the optimization problem are we talking about? Is it a maximization problem or minimization problem? If we are dealing with a minimization problem, why do we care about maxima? The first several paragraphs did not make the problem of interest clearer. But at least the fourth paragraph starts talking about training networks (the reviewer guesses this \"network\" refers to neural network, not other types network (e.g., Bayesian network) arising in machine learning). This paragraph talks about random initialization for minimizing a loss function, does this mean we are considering a minimization problem*s local maxima? In addition, random initialization-based neural network training algorithms like back propagation cannot guarantee giving local maxima or local minima of the problem of interest (which is the loss function for training). It is even not clear if a stationary point can be achieved. So if the method in this paper wishes to work with local maxima of an optimization problem, this may not be a proper example. The next paragraph brings out a notion of value function, which is hard to follow what it is. A suggestion is to give a much more concrete example to enlighten the readers. The next two paragraphs seem to be very disconnected. It is not properly defined what is x and how to obtain it. If they are local maxima of a problem, please give us an example: what is the optimization problem, and why this is an interesting setup? Since the problem setup of this paper is very hard to decode, it is also very hard to appreciate why the papers in the \"related work\" section are really related. The motivation and intuition behind the formulations in (1) and (2) are hard to follow, perhaps because the goal and objective of the paper is unclear. Overall, there is no formal problem definition or statement, and the notions and terminologies in this paper are not properly defined or introduced. This makes evaluating this work very hard. ========= after author feedback ======= After discussing with the authors through OpenReview, the reviewer feels that a lot of things have been clarified. The paper is interesting in its setting, and seems to be useful in different applications. The clarity can still be improved, but this might be more of a style matter. The analysis part is a bit heavy and overwhelming and not very insightful at this moment. Overall, the reviewer appreciate the effort for improving the readability of the paper and would like to change the recommendation to ```` accept.. ']\n",
      " ...\n",
      " ['Auxiliary Guided Autoregressive Variational Autoencoders. Generative modeling of high-dimensional data is a key problem in machine learning. Successful approaches include latent variable models and autoregressive models. The complementary strengths of these approaches, to model global and local image statistics respectively, suggest hybrid models combining the strengths of both models. Our contribution is to train such hybrid models using an auxiliary loss function that controls which information is captured by the latent variables and what is left to the autoregressive decoder. In contrast, prior work on such hybrid models needed to limit the capacity of the autoregressive decoder to prevent degenerate models that ignore the latent variables and only rely on autoregressive modeling. Our approach results in models with meaningful latent variable representations, and which rely on powerful autoregressive decoders to model image details. Our model generates qualitatively convincing samples, and yields state-of-the-art quantitative results.. Summary: This paper attempts to solve the problem of meaningfully combining variational autoencoders (VAEs) and PixelCNNs. It proposes to do this by simultaneously optimizing a VAE with PixelCNN++ decoder, and a VAE with factorial decoder. The model is evaluated in terms of log-likelihood (with no improvement over a PixelCNN++) and the visual appearance of samples and reconstructions. Review: Combining density networks (like VAEs) and autoregressive models is an unsolved problem and potentially very useful. To me, the most interesting bit of information in this paper was the realization that you can weight the reconstruction and KL terms of a VAE and interpret it as variational inference in a generative model with multiple copies of pixels (below Equation 7). Unfortunately the authors were unable to make any good use of this insight, and I will explain below why I don’t see any evidence of an improved generative model in this paper. As the paper is written now, it is not clear what the goal of the authors is. Is it density estimation? Then the addition of the VAE had no measurable effect on the PixelCNN++’s performance, i.e., it seems like a bad idea due to the added complexity and loss of tractability. Is it representation learning? Then the paper is missing experiments to support the idea that the learned representations are in any way an improvement. Is it image synthesis (not a real application by itself), then the paper should have demonstrated the usefulness of the model on a real task and probably involve human subjects in a quantitative evaluation. Much of the authors’ analysis is based on a qualitative evaluation of samples. However, samples can be very misleading. A lookup table storing the training data generates samples containing objects and perfect details, but obviously has not learned anything about either objects or the low-level statistics of natural images. In contrast to the authors, I fail to see a meaningful difference between the groups of samples in Figure 1. The VAE samples in Figure 3b) look quite smooth. Was independent Gaussian noise added to the VAE samples or are those (as is sometimes done) sampled means? If the former, what was sigma and how was it chosen? On page 7, the authors conclude that “the pixelCNN clearly takes into account the output of the VAE decoder” based on the samples. Being a mixture model, a PixelCNN++ could easily represent the following mixture: p(x | z) = 0.01 .prod_i p(x_i | x_{<i}) + 0.99 .prod_i p(x_i | z) The first term is just like a regular PixelCNN++, ignoring the latent variables. The second term is just like a variational autoencoder with factorial decoder. The samples in this case would be dominated by the VAE, which depends on the latent state. The log-likelihood would be dominated by the first term and would be minimally effected (see Theis et al., 2016). Note that I am not saying that this is exactly what the model has learned. I am merely providing a possible counter example to the notion that the PixelCNN++ has learned to use of the latent representation in a meaningful way. What happens if the KL term is simply downweighted but the factorial decoder is not included? This seems like it would be a useful control to include. The paper is well written and clear.. ']\n",
      " ['Learning to Augment Influential Data. Data augmentation is a technique to reduce overfitting and to improve generalization by increasing the number of labeled data samples by performing label preserving transformations; however, it is currently conducted in a trial and error manner. A composition of predefined transformations, such as rotation, scaling and cropping, is performed on training samples, and its effect on performance over test samples can only be empirically evaluated and cannot be predicted. This paper considers an influence function which predicts how generalization is affected by a particular augmented training sample in terms of validation loss. The influence function provides an approximation of the change in validation loss without comparing the performance which includes and excludes the sample in the training process. A differentiable augmentation model that generalizes the conventional composition of predefined transformations is also proposed. The differentiable augmentation model and reformulation of the influence function allow the parameters of the augmented model to be directly updated by backpropagation to minimize the validation loss. The experimental results show that the proposed method provides better generalization over conventional data augmentation methods.. This paper proposes an extension of the influence function study of Koh and Liang (2017) to data augmentation. Influence of augmentation, carried out via a parameterized and differentiable model, on validation loss is approximated and the augmentation model is learned under this approximation. Overall I think it is a valuable and publishable contribution. I do find the paper to be unclear and perhaps could be improved in a few ways. My main comments are: * The biggest question I have is it seems from Eq. 15 that authors are proposing an augmentation approach where the augmented samples replace the original samples and not co-exist with them in the training set. I am not sure why Eq. 15 has to be set up like that, please elaborate. * In Section 3.4 it is stated that only top fully connected layer of F is considered to compute influence function for augmentation. Does this also mean that when F is updated on augmented data only the top layer is updated? Please clarify. * The paper is a bit difficult to follow due to lack of clarity and few errors: - Section 2.1, Adversarial methods, “In these methods, a simple composition…adversarial examples” sentence is unclear - Page 2 footnote “however, they are referred to as unsupervised due to learning is not involved” sentence is unclear - Section 3.3 .tilde{z} in first line should be .tilde{z_i} - Eq. 15 LHS should include .tilde{z_i} - Section 3.4 “adopts” -> “adopt” - Section 3.4 “HVP” used without defining * Empirical evidence, while not extensive, is satisfactory.. ']\n",
      " ['The Incredible Shrinking Neural Network: New Perspectives on Learning Representations Through The Lens of Pruning. How much can pruning algorithms teach us about the fundamentals of learning representations in neural networks? A lot, it turns out. Neural network model compression has become a topic of great interest in recent years, and many different techniques have been proposed to address this problem. In general, this is motivated by the idea that smaller models typically lead to better generalization. At the same time, the decision of what to prune and when to prune necessarily forces us to confront our assumptions about how neural networks actually learn to represent patterns in data. In this work we set out to test several long-held hypotheses about neural network learning representations and numerical approaches to pruning. To accomplish this we first reviewed the historical literature and derived a novel algorithm to prune whole neurons (as opposed to the traditional method of pruning weights) from optimally trained networks using a second-order Taylor method. We then set about testing the performance of our algorithm and analyzing the quality of the decisions it made. As a baseline for comparison we used a first-order Taylor method based on the Skeletonization algorithm and an exhaustive brute-force serial pruning algorithm. Our proposed algorithm worked well compared to a first-order method, but not nearly as well as the brute-force method. Our error analysis led us to question the validity of many widely-held assumptions behind pruning algorithms in general and the trade-offs we often make in the interest of reducing computational complexity. We discovered that there is a straightforward way, however expensive, to serially prune 40-70\\\\% of the neurons in a trained network with minimal effect on the learning representation and without any re-training.. I did enjoy reading some of the introductions and background, in particular that of reminding readers of popular papers from the late 1980s and early 1990s. The idea of the proposal is straight forward: remove neurons based on the estimated change in the loss function from the packpropagation estimate with either first or second order backpropagation. The results are as expected that the first order method is worse then the second order method which in turn is worse than the brute force method. However, there are many reasons why I think that this work is not appropriate for ICLR. For one, there is now a much stronger comprehension of weight decay algorithms and their relation to Bayesian priors which has not been mentioned at all. I would think that any work in this regime would require at least some comments about this. Furthermore, there are many statements in the text that are not necessarily true, in particular in light of deep networks with modern regularization methods. For example, the authors state that the most accurate method is what they call brute-force. However, this assumes that the effects of each neurons are independent which might not be the case. So the serial order of removal is not necessarily the best. I also still think that this paper is unnecessarily long and the idea and the results could have been delivered in a much compressed way. I also don’t think just writing a Q&A section is not enough, and the points should be included in the paper.. The paper does not seem to have enough novelty, and the contribution is not clear enough due to presentation issues.']]\n",
      "['Accept' 'Reject' 'Accept' ... 'Reject' 'Reject' 'Reject']\n"
     ]
    }
   ],
   "source": [
    "# Allocating the attributes into feature variables(text) and target variables(acceptance_status) as x and y respectively\n",
    "x= dataframe4.iloc[:,:-3].values\n",
    "y= dataframe4.iloc[:,-1].values\n",
    "\n",
    "#Displaying the values of x and y\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0f4782c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Predicting Medications from Diagnostic Codes with Recurrent Neural Networks. It is a surprising fact that electronic medical records are failing at one of their primary purposes, that of tracking the set of medications that the patient is actively taking. Studies estimate that up to 50% of such lists omit active drugs, and that up to 25% of all active medications do not appear on the appropriate patient list. Manual efforts to maintain these lists involve a great deal of tedious human labor, which could be reduced by computational tools to suggest likely missing or incorrect medications on a patient’s list. We report here an application of recurrent neural networks to predict the likely therapeutic classes of medications that a patient is taking, given a sequence of the last 100 billing codes in their record. Our best model was a GRU that achieved high prediction accuracy (micro-averaged AUC 0.93, Label Ranking Loss 0.076), limited by hardware constraints on model size. Additionally, examining individual cases revealed that many of the predictions marked incorrect were likely to be examples of either omitted medications or omitted billing codes, supporting our assertion of a substantial number of errors and omissions in the data, and the likelihood of models such as these to help correct them.. This is a well-conducted and well-written study on the prediction of medication from diagnostic codes. The authors compared GRUs, LSTMs, feed-forward networks and random forests (making a case for why random forests should be used, instead of SVMs) and analysed the predictions and embeddings. The authors also did address the questions of the reviewers. My only negative point is that this work might be more relevant for a data science or medical venue rather than at ICLR.. This paper applies RNNs to predict medications from billing costs. While this paper does not have technical novelty, it is well done and well organized. It demonstrates a creative use of recent models in a very important domain, and I think many people in our community are interested and inspired by well-done applications that branch to socially important domains. Moreover, I think an advantage to accepting it at ICLR is that it gives our *expert* stamp of approval -- I see a lot of questionable / badly applied / antiquated machine learning methods in domain conferences, so I think it would be helpful for those domains to have examples of application papers that are considered sound.']\n",
      " ['Sufficient Conditions for Robustness to Adversarial Examples: a Theoretical and Empirical Study with Bayesian Neural Networks. We prove, under two sufficient conditions, that idealised models can have no adversarial examples. We discuss which idealised models satisfy our conditions, and show that idealised Bayesian neural networks (BNNs) satisfy these. We continue by studying near-idealised BNNs using HMC inference, demonstrating the theoretical ideas in practice. We experiment with HMC on synthetic data derived from MNIST for which we know the ground-truth image density, showing that near-perfect epistemic uncertainty correlates to density under image manifold, and that adversarial images lie off the manifold in our setting. This suggests why MC dropout, which can be seen as performing approximate inference, has been observed to be an effective defence against adversarial examples in practice; We highlight failure-cases of non-idealised BNNs relying on dropout, suggesting a new attack for dropout models and a new defence as well. Lastly, we demonstrate the defence on a cats-vs-dogs image classification task with a VGG13 variant.. In this paper, the authors posit a class of discriminative Bayesian classifiers that, under sufficient conditions, do not have any adversarial examples. They distinguish between two sources of uncertainty (epistemic and aleatoric), and show that mutual information is a measure of epistemic uncertainty (essentially uncertainty due to missing regions of the input space). They then define an idealised Bayesian Neural Network (BNN), which is essentially a BNN that 1) outputs the correct class probability (and always with probability 1.0) for each input in the training set (and for each transformation of training inputs that the data distribution is invariant under), and 2) outputs a sufficiently high uncertainty for each input not in the union of delta-balls surrounding the training set points. Similarly, an example is defined to be adversarial if it has two characteristics: it 1) lies far from the training data but is classified with high output probability, and it 2) is classified with high output probability although it lies very close to another example that is classified with high output probability for the other class. Condition 1) of an idealised BNN prevents Definition 2) of an adversarial example using the fact that BNNs are continuous, and Condition 2) prevents Definition 1) of an adversarial example since it will prevent \"garbage\" examples by predicting with high uncertainty (of course I*m glossing over many important technical details, but these are the main ideas if I understand correctly). The authors backed up their theoretical findings with empirical experiments. In the synthetic MNIST examples, the authors show that adversarial attacks are indeed correlated with lower true input probability. They show that training with HMC results in high uncertainty for inputs not near the input-space, a quality certainly not shared with all other deep models (and another reason that Bayesian models should be preferred for preventing adversarial attacks). On the other hand, the out-of-sample uncertainty for training with dropout is not sufficient to prevent adversarial attacks, although the authors posit a form of dropout ensemble training to help prevent these vulnerabilities. The authors are tackling an important issue with theoretical and technical tools that are not used often enough in machine learning research. Much of the literature on adversarial attacks is focused on finding adversarial examples, without trying to find a unifying theory for why they work. They do a very solid exposition of previous work, and one of the strengths of this paper comes in presenting their findings in the context of previously discovered adversarial attacks, in particular that of the spheres data set. Ultimately, I*m not convinced of the usefulness of their theoretical findings. In particular, the assumption that the model is invariant to all transformations that the data distribution is invariant under is an unprovable assumption that can expose many real-world vulnerabilities. This is the case of the spheres data set without a rotation invariance from Gilmer et al. (2018). In the appendix, the authors mention that the data invariance property is key for making the proof non-vacuous, and I would agree. Without the data invariance property, the proof mainly relies on the fact that BNNs are continuous. The experiments are promising in support of the theory, but they do not seem to address this data invariance property. Indeed a model can prevent adversarial examples by predicting high uncertainty for all points that are not near the training examples, which Bayesian models are well equipped to do. I also thought the paper was unclear at times. It is not easy to write such a technical and theoretical paper and to clearly convey all the main points, but I think the paper would*ve benefited from more clarity. For example, the notation is overloaded in a way that made it difficult to understand the main proofs, such as not clearly explaining what is meant by I(w ; p) and not contrasting between the binary entropy function used for the entropy of a constant epsilon and the more general random variable entropy function. In contrast, I thought the appendices were clearer and helpful for explaining the main ideas. Additionally, Lemma 1 follows trivially from continuity of a BNN. Perhaps removing this and being clearer with notation would*ve allowed for more room to be clearer for the proof of Theorem 1. A more minor point that I think would be interesting is comparing training with HMC to training with variational inference. Do the holes that come from training with dropout still exist for VI? VI could certainly scale in a way that HMC could not, which perhaps would make the results more applicable. Overall, this is a promising theoretical paper although I*m not currently convinced of the real-world applications beyond the somewhat small examples in the experiments section. PROS -Importance of the issue -Exposition and relation to previous work -Experimental results (although these were for smaller data sets) -Appendices really helped aid the understanding CONS -Real world usefulness -Clarity. ']\n",
      " ['Unsupervised Learning of the Set of Local Maxima. This paper describes a new form of unsupervised learning, whose input is a set of unlabeled points that are assumed to be local maxima of an unknown value function in an unknown subset of the vector space. Two functions are learned: (i) a set indicator , which is a binary classifier, and (ii) a comparator function that given two nearby samples, predicts which sample has the higher value of the unknown function . Loss terms are used to ensure that all training samples are a local maxima of , according to and satisfy . Therefore, and provide training signals to each other: a point in the vicinity of satisfies or is deemed by to be lower in value than . We present an algorithm, show an example where it is more efficient to use local maxima as an indicator function than to employ conventional classification, and derive a suitable generalization bound. Our experiments show that the method is able to outperform one-class classification algorithms in the task of anomaly detection and also provide an additional signal that is extracted in a completely unsupervised way.. The reviewer feels that the paper is hard to follow. The abstract is confusing enough and raises a number of questions. The paper talks about `\"local maxima\" without defining an optimization problem. What is the optimization problem are we talking about? Is it a maximization problem or minimization problem? If we are dealing with a minimization problem, why do we care about maxima? The first several paragraphs did not make the problem of interest clearer. But at least the fourth paragraph starts talking about training networks (the reviewer guesses this \"network\" refers to neural network, not other types network (e.g., Bayesian network) arising in machine learning). This paragraph talks about random initialization for minimizing a loss function, does this mean we are considering a minimization problem*s local maxima? In addition, random initialization-based neural network training algorithms like back propagation cannot guarantee giving local maxima or local minima of the problem of interest (which is the loss function for training). It is even not clear if a stationary point can be achieved. So if the method in this paper wishes to work with local maxima of an optimization problem, this may not be a proper example. The next paragraph brings out a notion of value function, which is hard to follow what it is. A suggestion is to give a much more concrete example to enlighten the readers. The next two paragraphs seem to be very disconnected. It is not properly defined what is x and how to obtain it. If they are local maxima of a problem, please give us an example: what is the optimization problem, and why this is an interesting setup? Since the problem setup of this paper is very hard to decode, it is also very hard to appreciate why the papers in the \"related work\" section are really related. The motivation and intuition behind the formulations in (1) and (2) are hard to follow, perhaps because the goal and objective of the paper is unclear. Overall, there is no formal problem definition or statement, and the notions and terminologies in this paper are not properly defined or introduced. This makes evaluating this work very hard. ========= after author feedback ======= After discussing with the authors through OpenReview, the reviewer feels that a lot of things have been clarified. The paper is interesting in its setting, and seems to be useful in different applications. The clarity can still be improved, but this might be more of a style matter. The analysis part is a bit heavy and overwhelming and not very insightful at this moment. Overall, the reviewer appreciate the effort for improving the readability of the paper and would like to change the recommendation to ```` accept.. ']\n",
      " ...\n",
      " ['Auxiliary Guided Autoregressive Variational Autoencoders. Generative modeling of high-dimensional data is a key problem in machine learning. Successful approaches include latent variable models and autoregressive models. The complementary strengths of these approaches, to model global and local image statistics respectively, suggest hybrid models combining the strengths of both models. Our contribution is to train such hybrid models using an auxiliary loss function that controls which information is captured by the latent variables and what is left to the autoregressive decoder. In contrast, prior work on such hybrid models needed to limit the capacity of the autoregressive decoder to prevent degenerate models that ignore the latent variables and only rely on autoregressive modeling. Our approach results in models with meaningful latent variable representations, and which rely on powerful autoregressive decoders to model image details. Our model generates qualitatively convincing samples, and yields state-of-the-art quantitative results.. Summary: This paper attempts to solve the problem of meaningfully combining variational autoencoders (VAEs) and PixelCNNs. It proposes to do this by simultaneously optimizing a VAE with PixelCNN++ decoder, and a VAE with factorial decoder. The model is evaluated in terms of log-likelihood (with no improvement over a PixelCNN++) and the visual appearance of samples and reconstructions. Review: Combining density networks (like VAEs) and autoregressive models is an unsolved problem and potentially very useful. To me, the most interesting bit of information in this paper was the realization that you can weight the reconstruction and KL terms of a VAE and interpret it as variational inference in a generative model with multiple copies of pixels (below Equation 7). Unfortunately the authors were unable to make any good use of this insight, and I will explain below why I don’t see any evidence of an improved generative model in this paper. As the paper is written now, it is not clear what the goal of the authors is. Is it density estimation? Then the addition of the VAE had no measurable effect on the PixelCNN++’s performance, i.e., it seems like a bad idea due to the added complexity and loss of tractability. Is it representation learning? Then the paper is missing experiments to support the idea that the learned representations are in any way an improvement. Is it image synthesis (not a real application by itself), then the paper should have demonstrated the usefulness of the model on a real task and probably involve human subjects in a quantitative evaluation. Much of the authors’ analysis is based on a qualitative evaluation of samples. However, samples can be very misleading. A lookup table storing the training data generates samples containing objects and perfect details, but obviously has not learned anything about either objects or the low-level statistics of natural images. In contrast to the authors, I fail to see a meaningful difference between the groups of samples in Figure 1. The VAE samples in Figure 3b) look quite smooth. Was independent Gaussian noise added to the VAE samples or are those (as is sometimes done) sampled means? If the former, what was sigma and how was it chosen? On page 7, the authors conclude that “the pixelCNN clearly takes into account the output of the VAE decoder” based on the samples. Being a mixture model, a PixelCNN++ could easily represent the following mixture: p(x | z) = 0.01 .prod_i p(x_i | x_{<i}) + 0.99 .prod_i p(x_i | z) The first term is just like a regular PixelCNN++, ignoring the latent variables. The second term is just like a variational autoencoder with factorial decoder. The samples in this case would be dominated by the VAE, which depends on the latent state. The log-likelihood would be dominated by the first term and would be minimally effected (see Theis et al., 2016). Note that I am not saying that this is exactly what the model has learned. I am merely providing a possible counter example to the notion that the PixelCNN++ has learned to use of the latent representation in a meaningful way. What happens if the KL term is simply downweighted but the factorial decoder is not included? This seems like it would be a useful control to include. The paper is well written and clear.. ']\n",
      " ['Learning to Augment Influential Data. Data augmentation is a technique to reduce overfitting and to improve generalization by increasing the number of labeled data samples by performing label preserving transformations; however, it is currently conducted in a trial and error manner. A composition of predefined transformations, such as rotation, scaling and cropping, is performed on training samples, and its effect on performance over test samples can only be empirically evaluated and cannot be predicted. This paper considers an influence function which predicts how generalization is affected by a particular augmented training sample in terms of validation loss. The influence function provides an approximation of the change in validation loss without comparing the performance which includes and excludes the sample in the training process. A differentiable augmentation model that generalizes the conventional composition of predefined transformations is also proposed. The differentiable augmentation model and reformulation of the influence function allow the parameters of the augmented model to be directly updated by backpropagation to minimize the validation loss. The experimental results show that the proposed method provides better generalization over conventional data augmentation methods.. This paper proposes an extension of the influence function study of Koh and Liang (2017) to data augmentation. Influence of augmentation, carried out via a parameterized and differentiable model, on validation loss is approximated and the augmentation model is learned under this approximation. Overall I think it is a valuable and publishable contribution. I do find the paper to be unclear and perhaps could be improved in a few ways. My main comments are: * The biggest question I have is it seems from Eq. 15 that authors are proposing an augmentation approach where the augmented samples replace the original samples and not co-exist with them in the training set. I am not sure why Eq. 15 has to be set up like that, please elaborate. * In Section 3.4 it is stated that only top fully connected layer of F is considered to compute influence function for augmentation. Does this also mean that when F is updated on augmented data only the top layer is updated? Please clarify. * The paper is a bit difficult to follow due to lack of clarity and few errors: - Section 2.1, Adversarial methods, “In these methods, a simple composition…adversarial examples” sentence is unclear - Page 2 footnote “however, they are referred to as unsupervised due to learning is not involved” sentence is unclear - Section 3.3 .tilde{z} in first line should be .tilde{z_i} - Eq. 15 LHS should include .tilde{z_i} - Section 3.4 “adopts” -> “adopt” - Section 3.4 “HVP” used without defining * Empirical evidence, while not extensive, is satisfactory.. ']\n",
      " ['The Incredible Shrinking Neural Network: New Perspectives on Learning Representations Through The Lens of Pruning. How much can pruning algorithms teach us about the fundamentals of learning representations in neural networks? A lot, it turns out. Neural network model compression has become a topic of great interest in recent years, and many different techniques have been proposed to address this problem. In general, this is motivated by the idea that smaller models typically lead to better generalization. At the same time, the decision of what to prune and when to prune necessarily forces us to confront our assumptions about how neural networks actually learn to represent patterns in data. In this work we set out to test several long-held hypotheses about neural network learning representations and numerical approaches to pruning. To accomplish this we first reviewed the historical literature and derived a novel algorithm to prune whole neurons (as opposed to the traditional method of pruning weights) from optimally trained networks using a second-order Taylor method. We then set about testing the performance of our algorithm and analyzing the quality of the decisions it made. As a baseline for comparison we used a first-order Taylor method based on the Skeletonization algorithm and an exhaustive brute-force serial pruning algorithm. Our proposed algorithm worked well compared to a first-order method, but not nearly as well as the brute-force method. Our error analysis led us to question the validity of many widely-held assumptions behind pruning algorithms in general and the trade-offs we often make in the interest of reducing computational complexity. We discovered that there is a straightforward way, however expensive, to serially prune 40-70\\\\% of the neurons in a trained network with minimal effect on the learning representation and without any re-training.. I did enjoy reading some of the introductions and background, in particular that of reminding readers of popular papers from the late 1980s and early 1990s. The idea of the proposal is straight forward: remove neurons based on the estimated change in the loss function from the packpropagation estimate with either first or second order backpropagation. The results are as expected that the first order method is worse then the second order method which in turn is worse than the brute force method. However, there are many reasons why I think that this work is not appropriate for ICLR. For one, there is now a much stronger comprehension of weight decay algorithms and their relation to Bayesian priors which has not been mentioned at all. I would think that any work in this regime would require at least some comments about this. Furthermore, there are many statements in the text that are not necessarily true, in particular in light of deep networks with modern regularization methods. For example, the authors state that the most accurate method is what they call brute-force. However, this assumes that the effects of each neurons are independent which might not be the case. So the serial order of removal is not necessarily the best. I also still think that this paper is unnecessarily long and the idea and the results could have been delivered in a much compressed way. I also don’t think just writing a Q&A section is not enough, and the points should be included in the paper.. The paper does not seem to have enough novelty, and the contribution is not clear enough due to presentation issues.']]\n",
      "[6. 5. 8. ... 5. 6. 3.]\n"
     ]
    }
   ],
   "source": [
    "# Allocating the attributes into feature variables(text) and target variables(review_score) as x and y_rs respectively\n",
    "x= dataframe4.iloc[:,:-3].values\n",
    "y_rs= dataframe4.iloc[:,-2].values\n",
    "\n",
    "#Displaying the values of x and y_rs\n",
    "print(x)\n",
    "print(y_rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6519368b",
   "metadata": {},
   "source": [
    "Text Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a661173",
   "metadata": {},
   "source": [
    "Here the below code will remove all the punctuations, stopwords, unnecessary spaces and any special characters from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e3147b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "\n",
    "# Importing WordNetLemmatizer from nltk.stem\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Creating object of WordNetLemmatizer\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "# Executing for loop to iterate through all the text and remove the unnecessary characters\n",
    "for sen in range(0, len(x)):\n",
    "    # this removes all the special characters\n",
    "    text = re.sub(r'\\W', ' ', str(x[sen]))\n",
    "    \n",
    "    # this removes all single characters\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "    \n",
    "    # this removes single characters from the start\n",
    "    text = re.sub(r'\\^[a-zA-Z]\\s+', ' ', text) \n",
    "    \n",
    "    # this substitutes all multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "    \n",
    "    # this removes prefixed 'b'\n",
    "    text = re.sub(r'^b\\s+', '', text)\n",
    "    \n",
    "    # this converts all the texts to Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    text = text.split()\n",
    "\n",
    "    text = [stemmer.lemmatize(word) for word in text]\n",
    "    text = ' '.join(text)\n",
    "    \n",
    "    texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1ec13941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing CountVectorizer class from sklearn.feature_extraction.text library\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# this code converts all the texts into corresponding numerical features\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "x = vectorizer.fit_transform(texts).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ad1134ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing TfidTransformer class from sklearn.feature_extraction.text library\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# this code converts all the text into TFID feature values\n",
    "# TFID stands for term frequency and Inverse Document frequency\n",
    "tfidfconverter = TfidfTransformer()\n",
    "x = tfidfconverter.fit_transform(x).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f758005",
   "metadata": {},
   "source": [
    "Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "405d7043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing train_test_split utility from sklearn.model_selection \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the dataset into training and test set\n",
    "# here training set contains 80% of the data whereas test set contains just 20% of the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "X_train_rs, X_test_rs, y_train_rs, y_test_rs = train_test_split(x,y_rs, test_size=0.2, random_state=0) # for review_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b369a21b",
   "metadata": {},
   "source": [
    "Training the Models ( Linear Support Vector Machine and Random Forest Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8311f9cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the Support Vector Machine from sklearn\n",
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel='linear') # for acceptance_status\n",
    "clf_rs = svm.SVC(kernel='linear') # for review_score\n",
    "\n",
    "# Training the model with training data\n",
    "clf.fit(X_train, y_train)\n",
    "clf_rs.fit(X_train_rs, y_train_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "00bd9cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=1000, random_state=0)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing RandomForestClassifier from sklearn.ensemble library\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0) # for acceptance_status\n",
    "classifier_rs = RandomForestClassifier(n_estimators=1000, random_state=0) # for review_score\n",
    "\n",
    "# Training the Algorithm with training dataset\n",
    "classifier.fit(X_train, y_train) # for acceptance_status\n",
    "classifier_rs.fit(X_train_rs, y_train_rs) # for review_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5251c459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the target value for test set using SVM Model\n",
    "y_pred1 = clf.predict(X_test) # for acceptance_status\n",
    "y_pred1_rs = clf_rs.predict(X_test_rs) # for review_score\n",
    "\n",
    "# predicting the target value for the test set using Random Forest Classifier Model\n",
    "y_pred2 = classifier.predict(X_test) # for acceptance_status\n",
    "y_pred2_rs = classifier_rs.predict(X_test_rs) # for review_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4025159",
   "metadata": {},
   "source": [
    "Evaluating Both the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6f4171a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrices for SVM Model for attribute acceptance_status\n",
      "[[109 128]\n",
      " [ 61 301]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Accept       0.64      0.46      0.54       237\n",
      "      Reject       0.70      0.83      0.76       362\n",
      "\n",
      "    accuracy                           0.68       599\n",
      "   macro avg       0.67      0.65      0.65       599\n",
      "weighted avg       0.68      0.68      0.67       599\n",
      "\n",
      "0.6844741235392321\n",
      "-------------------------------------------------------\n",
      "Evaluation metrices for SVM Model for attribute review_score\n",
      "[[ 0  1  3  6  2  2  0  0]\n",
      " [ 0  2 15 15  8  8  0  0]\n",
      " [ 0  2 26 41 37 11  0  0]\n",
      " [ 0  1 27 28 53 12  0  0]\n",
      " [ 0  1 31 31 57 19  0  0]\n",
      " [ 0  2 15 25 44 31  0  0]\n",
      " [ 0  0  6  4 14 12  0  0]\n",
      " [ 0  0  2  0  2  3  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         2.0       0.00      0.00      0.00        14\n",
      "         3.0       0.22      0.04      0.07        48\n",
      "         4.0       0.21      0.22      0.21       117\n",
      "         5.0       0.19      0.23      0.21       121\n",
      "         6.0       0.26      0.41      0.32       139\n",
      "         7.0       0.32      0.26      0.29       117\n",
      "         8.0       0.00      0.00      0.00        36\n",
      "         9.0       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.24       599\n",
      "   macro avg       0.15      0.15      0.14       599\n",
      "weighted avg       0.22      0.24      0.22       599\n",
      "\n",
      "0.24040066777963273\n",
      "-------------------------------------------------------\n",
      "Evaluation metrices for Random Forest Classifier Model for attribute acceptance_status\n",
      "[[ 65 172]\n",
      " [  9 353]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Accept       0.88      0.27      0.42       237\n",
      "      Reject       0.67      0.98      0.80       362\n",
      "\n",
      "    accuracy                           0.70       599\n",
      "   macro avg       0.78      0.62      0.61       599\n",
      "weighted avg       0.75      0.70      0.65       599\n",
      "\n",
      "0.6978297161936561\n",
      "---------------------------------------------------------\n",
      "Evaluation metrices for Random Forest Classifier Model for attribute review_score\n",
      "[[ 0  0  4  3  5  2  0  0]\n",
      " [ 0  0 25 11 10  2  0  0]\n",
      " [ 0  0 46 25 40  6  0  0]\n",
      " [ 0  0 38 19 60  4  0  0]\n",
      " [ 0  0 32 21 76 10  0  0]\n",
      " [ 0  1 10  8 77 21  0  0]\n",
      " [ 0  0  6  2 20  8  0  0]\n",
      " [ 0  0  0  0  4  3  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         2.0       0.00      0.00      0.00        14\n",
      "         3.0       0.00      0.00      0.00        48\n",
      "         4.0       0.29      0.39      0.33       117\n",
      "         5.0       0.21      0.16      0.18       121\n",
      "         6.0       0.26      0.55      0.35       139\n",
      "         7.0       0.38      0.18      0.24       117\n",
      "         8.0       0.00      0.00      0.00        36\n",
      "         9.0       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.27       599\n",
      "   macro avg       0.14      0.16      0.14       599\n",
      "weighted avg       0.23      0.27      0.23       599\n",
      "\n",
      "0.2704507512520868\n"
     ]
    }
   ],
   "source": [
    "# Importing utilities Classification_report, confusion_matrix and accuracy_score from sklearn.metrics library\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Displaying the results of Evaluation metrices for SVM Model for acceptance_status\n",
    "print('Evaluation metrices for SVM Model for attribute acceptance_status')\n",
    "print(confusion_matrix(y_test,y_pred1))\n",
    "print(classification_report(y_test,y_pred1, zero_division=0))\n",
    "print(accuracy_score(y_test, y_pred1))\n",
    "print('-------------------------------------------------------')\n",
    "\n",
    "# Displaying the results of Evaluation metrices for SVM Model for review_score\n",
    "print('Evaluation metrices for SVM Model for attribute review_score')\n",
    "print(confusion_matrix(y_test_rs,y_pred1_rs))\n",
    "print(classification_report(y_test_rs,y_pred1_rs, zero_division=0))\n",
    "print(accuracy_score(y_test_rs, y_pred1_rs))\n",
    "print('-------------------------------------------------------')\n",
    "\n",
    "# Displaying the results of Evaluation metrices for Random Forest Classifier Model for acceptance_status\n",
    "print('Evaluation metrices for Random Forest Classifier Model for attribute acceptance_status')\n",
    "print(confusion_matrix(y_test,y_pred2))\n",
    "print(classification_report(y_test,y_pred2, zero_division=0))\n",
    "print(accuracy_score(y_test, y_pred2))\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# Displaying the results of Evaluation metrices for Random Forest Classifier Model for review_score\n",
    "print('Evaluation metrices for Random Forest Classifier Model for attribute review_score')\n",
    "print(confusion_matrix(y_test_rs,y_pred2_rs))\n",
    "print(classification_report(y_test_rs,y_pred2_rs, zero_division=0))\n",
    "print(accuracy_score(y_test_rs, y_pred2_rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143c08ec",
   "metadata": {},
   "source": [
    "Validation of the Models for predicting both the attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "aea61081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2392, 1500)\n",
      "(300, 1500)\n",
      "(300, 1500)\n",
      "----------------------------------------------------------------------------\n",
      "(2392, 1500)\n",
      "(300, 1500)\n",
      "(300, 1500)\n"
     ]
    }
   ],
   "source": [
    "# Creating a Validation set and validating the model\n",
    "train_ratio = 0.80\n",
    "test_ratio = 0.10\n",
    "validation_ratio = 0.10\n",
    "\n",
    "# Creating training and test set for both attributes\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=test_ratio) # for acceptance_status\n",
    "X_train_rs, X_test_rs, y_train_rs, y_test_rs = train_test_split(x, y_rs, test_size=test_ratio) # for review_score\n",
    "\n",
    "# Creating training and validation set for both attributes\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=validation_ratio/(train_ratio+test_ratio)) # for acceptance_status\n",
    "X_train_rs, X_valid_rs, y_train_rs, y_valid_rs = train_test_split(X_train_rs, y_train_rs, test_size=validation_ratio/(train_ratio+test_ratio)) # for review_score\n",
    "\n",
    "# displaying the shape of training, test and Validation set for attribute acceptance_status\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_valid.shape)\n",
    "print('----------------------------------------------------------------------------')\n",
    "\n",
    "# displaying the shape of training, test and Validation set for attribute review_score\n",
    "print(X_train_rs.shape)\n",
    "print(X_test_rs.shape)\n",
    "print(X_valid_rs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0d7e6ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting the target values for the Validation set \n",
    "\n",
    "# for SVM Model\n",
    "y_pred3 = clf.predict(X_valid) # for acceptance_status\n",
    "y_pred3_rs = clf_rs.predict(X_valid_rs) # for review_score\n",
    "\n",
    "# for Random Forest Classifier\n",
    "y_pred4 = classifier.predict(X_valid) # for acceptance_status\n",
    "y_pred4_rs = classifier_rs.predict(X_valid_rs) # for review_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4b8fbc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrices for validation set of attribute acceptance_status using SVM Model\n",
      "[[ 82  39]\n",
      " [ 20 159]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Accept       0.80      0.68      0.74       121\n",
      "      Reject       0.80      0.89      0.84       179\n",
      "\n",
      "    accuracy                           0.80       300\n",
      "   macro avg       0.80      0.78      0.79       300\n",
      "weighted avg       0.80      0.80      0.80       300\n",
      "\n",
      "0.8033333333333333\n",
      "------------------------------------------------------------\n",
      "Evaluation metrices for validation set of attribute review_score using SVM Model\n",
      "[[ 0  0  0  0  1  0  1  0  0]\n",
      " [ 0  0  0  1  1  0  0  0  0]\n",
      " [ 0  0  8  9  3  5  1  0  0]\n",
      " [ 0  0  0 41 12 13  3  0  0]\n",
      " [ 0  0  0  9 29 12  5  0  0]\n",
      " [ 0  0  0  8  6 50  5  0  0]\n",
      " [ 0  0  1  6  9 10 28  0  0]\n",
      " [ 0  0  0  3  4  6  3  1  0]\n",
      " [ 0  0  0  0  0  2  3  0  1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.00      0.00      0.00         2\n",
      "         2.0       0.00      0.00      0.00         2\n",
      "         3.0       0.89      0.31      0.46        26\n",
      "         4.0       0.53      0.59      0.56        69\n",
      "         5.0       0.45      0.53      0.48        55\n",
      "         6.0       0.51      0.72      0.60        69\n",
      "         7.0       0.57      0.52      0.54        54\n",
      "         8.0       1.00      0.06      0.11        17\n",
      "         9.0       1.00      0.17      0.29         6\n",
      "\n",
      "    accuracy                           0.53       300\n",
      "   macro avg       0.55      0.32      0.34       300\n",
      "weighted avg       0.58      0.53      0.51       300\n",
      "\n",
      "0.5266666666666666\n",
      "------------------------------------------------------------\n",
      "Evaluation metrices for validation set of attribute acceptance_status using Random Forest Classifier\n",
      "[[101  20]\n",
      " [  0 179]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Accept       1.00      0.83      0.91       121\n",
      "      Reject       0.90      1.00      0.95       179\n",
      "\n",
      "    accuracy                           0.93       300\n",
      "   macro avg       0.95      0.92      0.93       300\n",
      "weighted avg       0.94      0.93      0.93       300\n",
      "\n",
      "0.9333333333333333\n",
      "--------------------------------------------------------------\n",
      "Evaluation metrices for validation set of attribute review_score using Random Forest Classifier\n",
      "[[ 2  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  1  0  0  0]\n",
      " [ 0  0 19  5  0  2  0  0  0]\n",
      " [ 0  0  0 56  5  7  1  0  0]\n",
      " [ 0  0  0  6 41  8  0  0  0]\n",
      " [ 0  0  0  3  3 60  3  0  0]\n",
      " [ 0  0  0  2  0  5 47  0  0]\n",
      " [ 0  0  0  2  0  2  0 13  0]\n",
      " [ 0  0  0  0  0  1  0  0  5]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00         2\n",
      "         2.0       1.00      0.50      0.67         2\n",
      "         3.0       1.00      0.73      0.84        26\n",
      "         4.0       0.76      0.81      0.78        69\n",
      "         5.0       0.84      0.75      0.79        55\n",
      "         6.0       0.70      0.87      0.77        69\n",
      "         7.0       0.92      0.87      0.90        54\n",
      "         8.0       1.00      0.76      0.87        17\n",
      "         9.0       1.00      0.83      0.91         6\n",
      "\n",
      "    accuracy                           0.81       300\n",
      "   macro avg       0.91      0.79      0.84       300\n",
      "weighted avg       0.83      0.81      0.82       300\n",
      "\n",
      "0.8133333333333334\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Displaying the results of Evaluation metrices for validation set using SVM Model for acceptance_status\n",
    "print('Evaluation metrices for validation set of attribute acceptance_status using SVM Model')\n",
    "print(confusion_matrix(y_valid,y_pred3))\n",
    "print(classification_report(y_valid,y_pred3, zero_division=0))\n",
    "print(accuracy_score(y_valid, y_pred3))\n",
    "print('------------------------------------------------------------')\n",
    "\n",
    "# Displaying the results of Evaluation metrices for validation set using SVM Model for review_score\n",
    "print('Evaluation metrices for validation set of attribute review_score using SVM Model')\n",
    "print(confusion_matrix(y_valid_rs,y_pred3_rs))\n",
    "print(classification_report(y_valid_rs,y_pred3_rs, zero_division=0))\n",
    "print(accuracy_score(y_valid_rs, y_pred3_rs))\n",
    "print('------------------------------------------------------------')\n",
    "\n",
    "# Displaying the results of Evaluation metrices for validation set using Random Forest Classifier for acceptance_status\n",
    "print('Evaluation metrices for validation set of attribute acceptance_status using Random Forest Classifier')\n",
    "print(confusion_matrix(y_valid,y_pred4))\n",
    "print(classification_report(y_valid,y_pred4, zero_division=0))\n",
    "print(accuracy_score(y_valid, y_pred4))\n",
    "print('--------------------------------------------------------------')\n",
    "\n",
    "# Displaying the results of Evaluation metrices for validation set using Random Forest Classifier for review_score\n",
    "print('Evaluation metrices for validation set of attribute review_score using Random Forest Classifier')\n",
    "print(confusion_matrix(y_valid_rs,y_pred4_rs))\n",
    "print(classification_report(y_valid_rs,y_pred4_rs, zero_division=0))\n",
    "print(accuracy_score(y_valid_rs, y_pred4_rs))\n",
    "print('--------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d395cfac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
